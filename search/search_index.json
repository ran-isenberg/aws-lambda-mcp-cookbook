{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homepage","text":""},{"location":"#aws-aws-lambda-mcp-cookbook-a-serverless-mcp-server-blueprint","title":"AWS AWS Lambda MCP Cookbook - a Serverless MCP Server Blueprint","text":""},{"location":"#the-problem","title":"The Problem","text":"<p>Starting a production grade Serverless MCP can be overwhelming. You need to figure out many questions and challenges that have nothing to do with your business domain:</p> <ul> <li>How to deploy to the cloud? What IAC framework do you choose?</li> <li>How to write a SaaS-oriented CI/CD pipeline? What does it need to contain?</li> <li>How do you handle observability, logging, tracing, metrics?</li> <li>How do you write a production grade Lambda function?</li> <li>How do you handle testing?</li> <li>What makes an AWS Lambda handler resilient, traceable, and easy to maintain? How do you write such a code?</li> </ul>"},{"location":"#the-solution","title":"The Solution","text":"<p>This project aims to reduce cognitive load and answer these questions for you by providing a skeleton Python Serverless service blueprint that implements best practices for AWS Lambda, Serverless CI/CD, and AWS CDK in one blueprint project.</p> <p>This project is a blueprint for new Serverless MCP servers.</p> <p>It provides two implementation options:</p> <ol> <li>Pure, native Lambda function with no FastMCP.</li> <li>Lambda with AWS web adapter and FastMCP</li> </ol> <p>Choose the architecture that you see fit, each with its own pros and cons.</p> <p></p>"},{"location":"#option-1-serverless-native-lambda-mcp-server","title":"Option 1: Serverless Native Lambda MCP Server","text":"<p>This project provides a working, open source based, AWS Lambda based Python MCP server implementation.</p> <p>The MCP server uses JSON RPC over HTTP (non streamable) via API Gateway's body payload parameter. See integration tests and see how the test event is generated.</p> <p>It contains an advanced implementation including IaC CDK code and a CI/CD pipeline, testing, observability and more (see Features section).</p> <p>It's started based on AWS sample for MCP - but had major refactors since, combined with the AWS Lambda Handler cookbook template.</p> <p>Better fitted for POCs or tool oriented MCPs. Can be secured with custom authentication code and WAF.</p>"},{"location":"#option-2-serverless-lambda-web-adapter-fastmcp","title":"Option 2: Serverless Lambda Web Adapter &amp; FastMCP","text":"<p>Based on AWS Web Adapter and FastMCP.</p> <p>Use an HTTP API GW and Lambda function. Can be used with a REST API GW with a custom domain too.</p> <p>Better fitted for production-grade MCP servers as it upholds to the official MCP protocol and has native auth mechanism (OAuth).</p>"},{"location":"#monitoring-design","title":"Monitoring Design","text":""},{"location":"#features","title":"Features","text":"<ul> <li>PURE Lambda - not web adapter, no FastMCP required or Web adapter with FastMCP.</li> <li>Python Serverless MCP server with a recommended file structure.</li> <li>MCP Tools input validation: check argument types and values</li> <li>CDK infrastructure with infrastructure tests and security tests.</li> <li>Tests - unit, integration (tests for full MCP messages) and E2E with a real MCP client</li> <li>CI/CD pipelines based on Github actions that deploys to AWS with python linters, complexity checks and style formatters.</li> <li>CI/CD pipeline deploys to dev/staging and production environments with different gates between each environment</li> <li>Makefile for simple developer experience.</li> <li>The AWS Lambda handler embodies Serverless best practices and has all the bells and whistles for a proper production ready handler.</li> <li>AWS Lambda handler uses AWS Lambda Powertools.</li> <li>AWS Lambda handler 3 layer architecture: handler layer, logic layer and data access layer</li> <li>Session context storage in DynamoDB - global getter and setter (get_session, set_session) - be advised, has security issue - need to match session id to user</li> <li>API protected by WAF with four AWS managed rules in production deployment</li> <li>CloudWatch dashboards - High level and low level including CloudWatch alarms</li> </ul> <p>The GitHub blueprint project can be found at https://github.com/ran-isenberg/aws-lambda-mcp-cookbook.</p>"},{"location":"#serverless-best-practices","title":"Serverless Best Practices","text":"<p>The AWS Lambda handler will implement multiple best practice utilities.</p> <p>Each utility is implemented when a new blog post is published about that utility.</p> <p>The utilities cover multiple aspects of a production-ready service, including:</p> <ul> <li>Logging</li> <li>Observability: Monitoring and Tracing</li> <li>Observability: Business KPI Metrics</li> <li>Environment Variables</li> <li>Hexagonal Architecture</li> <li>Input Validation</li> <li>Serverless Monitoring</li> <li>Learn How to Write AWS Lambda Functions with Three Architecture Layers</li> </ul> <p>While the code examples are written in Python, the principles are valid to any supported AWS Lambda handler programming language.</p>"},{"location":"#security","title":"Security","text":"<p>For pure Lambda:</p> <ul> <li>WAF connected in production accounts (requires having an environment variable during deployment called 'ENVIRONMENT' with a value of 'production')</li> <li>Auth/Authz function placeholder in the mcp.py handler function - see authentication.py</li> <li>It is recommended to either use IAM/Cognito/Lambda authorizer or use the authentication.py and implement identity provider token validation flow.</li> </ul> <p>For FastMCP:</p> <ul> <li>Use FastMCP Auth parameter for Oauth implementation.</li> <li>If you use session id management, you need to make sure the session id matches the user id by yourself.</li> </ul>"},{"location":"#known-issues","title":"Known Issues","text":"<ul> <li>There might be security issues with this implementation, MCP is very new and has many issues.</li> <li>Session saving - there's no match validation between session id and user id/tenant id. This is a TODO item.</li> <li>It is not possible to manually update session data, only fetch.</li> <li>Pure Lambda variation has limited MCP protocol support, it's based used for tools only simple MCP. For full blown services, use the FastMCP variation.</li> </ul>"},{"location":"#handler-examples","title":"Handler Examples","text":"<p>Pure Lambda:</p> service/handlers/mcp.py<pre><code>from aws_lambda_env_modeler import init_environment_variables\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nfrom service.handlers.models.env_vars import McpHandlerEnvVars\nfrom service.handlers.utils.authentication import authenticate\nfrom service.handlers.utils.mcp import mcp\nfrom service.handlers.utils.observability import logger, metrics, tracer\nfrom service.logic.tools.math import add_two_numbers\n\n\n@mcp.tool()\ndef math(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers together\"\"\"\n    if not isinstance(a, int) or not isinstance(b, int):\n        raise ValueError('Invalid input: a and b must be integers')\n\n    # Uncomment the following line if you want to use session data\n    # session_data: Optional[SessionData] = mcp.get_session()\n\n    # call logic layer\n    result = add_two_numbers(a, b)\n\n    # save session data\n    mcp.set_session(data={'result': result})\n\n    metrics.add_metric(name='ValidMcpEvents', unit=MetricUnit.Count, value=1)\n    return result\n\n\n@init_environment_variables(model=McpHandlerEnvVars)\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@metrics.log_metrics\n@tracer.capture_lambda_handler(capture_response=False)\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    authenticate(event, context)\n    return mcp.handle_request(event, context)\n</code></pre> <p>Handler is found at service/handlers/mcp.py</p> <p>MCP engine found at service/mcp_lambda_handler folder</p> <p>FastMCP Lambda:</p> service/mcp_server.py<pre><code>from fastmcp import FastMCP\n\nfrom service.handlers.utils.observability import logger\nfrom service.logic.prompts.hld import hld_prompt\nfrom service.logic.resources.profiles import get_profile_by_id\nfrom service.logic.tools.math import add_two_numbers\n\nmcp: FastMCP = FastMCP(name='mcp-lambda-server')\n\n\n@mcp.tool\ndef math(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers together\"\"\"\n    logger.info('using math tool', extra={'a': a, 'b': b})\n    return add_two_numbers(a, b)\n\n\n# Dynamic resource template\n@mcp.resource('users://{user_id}/profile')\ndef get_profile(user_id: int) -&gt; dict[str, str]:\n    \"\"\"Fetch user profile by user ID.\"\"\"\n    logger.info('fetching user profile', extra={'user_id': user_id})\n    return get_profile_by_id(user_id)\n\n\n@mcp.prompt()\ndef generate_serverless_design_prompt(design_requirements: str) -&gt; str:\n    \"\"\"Generate a serverless design prompt based on the provided design requirements.\"\"\"\n    logger.info('generating serverless design prompt', extra={'design_requirements': design_requirements})\n    return hld_prompt(design_requirements)\n\n\napp = mcp.http_app(transport='http', stateless_http=True, json_response=True)\n</code></pre> <p>Handler is found at service/mcp_server.py</p>"},{"location":"#license","title":"License","text":"<p>This library is licensed under the MIT License. See the LICENSE file.</p>"},{"location":"cdk/","title":"AWS CDK","text":""},{"location":"cdk/#prerequisites","title":"Prerequisites","text":"<ul> <li>Follow this getting started with CDK guide</li> <li>Make sure your AWS account and machine can deploy an AWS Cloudformation stack and have all the tokens and configuration as described in the page above.</li> <li>CDK Best practices blog</li> <li>Lambda layers best practices blog</li> </ul>"},{"location":"cdk/#cdk-deployment","title":"CDK Deployment","text":"<p>All CDK project files can be found under the CDK folder.</p> <p>The CDK code create an API GW with a path of /api/orders which triggers the lambda on 'POST' requests.</p> <p>The AWS Lambda handler uses a Lambda layer optimization which takes all the packages under the [packages] section in the Pipfile and downloads them in via a Docker instance.</p> <p>This allows you to package any custom dependencies you might have.</p> <p>In order to add a new dev dependency, add it to the Pipfile under the [tool.poetry.dependencies] section and run <code>poetry update -vvv</code>.</p> <p>In order to add a new Lambda runtime dependency, add it to the Pipfile under the [tool.poetry.dependencies] section and run <code>poetry update -vvv</code>.</p>"},{"location":"cdk/#cdk-constants","title":"CDK Constants","text":"<p>All AWS Lambda function configurations are saved as constants at the <code>cdk.service.constants.py</code> file and can easily be changed.</p> <ul> <li>Memory size</li> <li>Timeout in seconds</li> <li>Lambda dependencies build folder location</li> <li>Lambda Layer dependencies build folder location</li> <li>Various resources names</li> <li>Lambda function environment variables names and values</li> </ul>"},{"location":"cdk/#deployed-resources","title":"Deployed Resources","text":"<ul> <li>AWS Cloudformation stack: cdk.service.service_stack.py which is consisted of one construct</li> <li>Construct: cdk.service.api_construct.py which includes:<ul> <li>Lambda Layer - deployment optimization meant to be used with multiple handlers under the same API GW, sharing code logic and dependencies. You can read more about it here.</li> <li>Lambda Function - The Lambda handler function itself. Handler code is taken from the service <code>folder</code>.</li> <li>Lambda Role - The role of the Lambda function.</li> <li>API GW with Lambda Integration - API GW with a Lambda integration POST /api/orders that triggers the Lambda function.</li> <li>AWS DynamoDB table - stores request data. Created in the <code>api_db_construct.py</code> construct.</li> <li>AWS DynamoDB table - stores idempotency data. Created in the <code>api_db_construct.py</code> construct.</li> </ul> </li> </ul>"},{"location":"cdk/#infrastructure-cdk-security-tests","title":"Infrastructure CDK &amp; Security Tests","text":"<p>Under tests there is an <code>infrastructure</code> folder for CDK infrastructure tests.</p> <p>The first test, <code>test_cdk</code> uses CDK's testing framework which asserts that required resources exists so the application will not break anything upon deployment.</p> <p>The security tests are based on <code>cdk_nag</code>. It checks your cloudformation output for security best practices. It can be found in the <code>service_stack.py</code> as part of the stack definition. It will fail the deployment when there is a security issue.</p> <p>For more information click here.</p>"},{"location":"cdk/#deployed-resources_1","title":"Deployed Resources","text":"<p>In the picture below you can see all the deployed resources ordered into domain groups. The image was created with the IDE plugin of AWS Application Composer.</p> <p></p>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker - install Docker. Required for the Lambda layer packaging process.</li> <li>AWS CDK - Required for synth &amp; deploying the AWS Cloudformation stack. Run CDK Bootstrap on your AWS account and region.</li> <li>Python 3.13</li> <li>poetry - Make sure to have poetry v2 and above and to run <code>poetry config --local virtualenvs.in-project true</code> so all dependencies are installed in the project '.venv' folder.</li> <li>For Windows based machines, use the Makefile_windows version (rename to Makefile). Default Makefile is for Mac/Linux.</li> </ul>"},{"location":"getting_started/#getting-started","title":"Getting Started","text":"<p>You can start with a clean service out of this blueprint repository without using the 'Template' button on GitHub.</p> <p>That's it, you are ready to deploy the MCP server:</p> <pre><code>cd {new repo folder}\npoetry env activate\npoetry install\nmake deploy\n</code></pre> <p>Make sure you have poetry v2 and above.</p> <p>You can also run 'make pr' will run all checks, synth, file formatters , unit tests, deploy to AWS and run integration and E2E tests.</p> <ol> <li>Run <code>make dev</code></li> </ol>"},{"location":"getting_started/#deploy-cdk","title":"Deploy CDK","text":"<p>Create a cloudformation stack by running <code>make deploy</code>.</p>"},{"location":"getting_started/#unit-tests","title":"Unit Tests","text":"<p>Unit tests can be found under the <code>tests/unit</code> folder.</p> <p>You can run the tests by using the following command: <code>make unit</code>.</p>"},{"location":"getting_started/#integration-tests","title":"Integration Tests","text":"<p>Make sure you deploy the stack first as these tests trigger your lambda handler LOCALLY but they can communicate with AWS services.</p> <p>These tests allow you to debug in your IDE your AWS Lambda function.</p> <p>Integration tests can be found under the <code>tests/integration</code> folder.</p> <p>You can run the tests by using the following command: <code>make integration</code>.</p>"},{"location":"getting_started/#e2e-tests","title":"E2E Tests","text":"<p>Make sure you deploy the stack first.</p> <p>E2E tests can be found under the <code>tests/e2e</code> folder.</p> <p>These tests send a 'POST' message to the deployed API GW and trigger the Lambda function on AWS.</p> <p>The tests are run automatically by: <code>make e2e</code>.</p>"},{"location":"getting_started/#deleting-the-stack","title":"Deleting the stack","text":"<p>CDK destroy can be run with <code>make destroy</code>.</p>"},{"location":"getting_started/#preparing-code-for-pr","title":"Preparing Code for PR","text":"<p>Run <code>make pr</code>. This command will run all the required checks, pre commit hooks, linters, code formatters, import sorting and tests, so you can be sure GitHub's pipeline will pass. It will also generate an updated swagger OpenAPI JSON file and place it at docs/swagger/openapi.json location.</p> <p>The command auto fixes errors in the code for you.</p> <p>If there's an error in the pre-commit stage, it gets auto fixed. However, are required to run <code>make pr</code> again so it continues to the next stages.</p> <p>Be sure to commit all the changes that <code>make pr</code> does for you.</p>"},{"location":"getting_started/#github-pages-documentation","title":"GitHub Pages Documentation","text":"<p><code>make docs</code> can be run to start a local HTTP server with the project's documentation pages.</p>"},{"location":"getting_started/#building-devlambda_requirementstxt","title":"Building dev/lambda_requirements.txt","text":""},{"location":"getting_started/#lambda_requirementstxt","title":"lambda_requirements.txt","text":"<p>CDK requires a requirements.txt in order to create a zip file with the Lambda layer dependencies. It's based on the project's poetry.lock file.</p> <p><code>make deploy</code> command will generate it automatically for you.</p>"},{"location":"getting_started/#dev_requirementstxt","title":"dev_requirements.txt","text":"<p>This file is used during GitHub CI to install all the required Python libraries without using poetry.</p> <p>File contents are created out of the Pipfile.lock.</p> <p><code>make deploy</code> and <code>make deps</code> are commands generate it automatically.</p>"},{"location":"pipeline/","title":"CI/CD Pipeline","text":""},{"location":"pipeline/#getting-started","title":"Getting Started","text":"<p>The GitHub CI/CD pipeline includes the following steps.</p> <p>The pipelines uses environment secrets (under the defined environment 'dev', 'staging' and 'production') for code coverage and for the role to deploy to AWS.</p> <p>When you clone this repository be sure to define the environments in your repo settings and create a secret per environment:</p> <ul> <li>AWS_ROLE - to role to assume for your GitHub worker as defined here</li> </ul>"},{"location":"pipeline/#makefile-commands","title":"Makefile Commands","text":"<p>All steps can be run locally using the makefile. See details below:</p> <ul> <li>Create Python environment</li> <li>Install dev dependencies</li> <li>Run pre-commit checks as defined in <code>.pre-commit-config.yaml</code></li> <li>Lint and format and sort imports with ruff (similar to flake8/yapf/isort) - run <code>make format</code> in the IDE</li> <li>Static type check with mypy as defined in <code>.mypy.ini</code> - run <code>make lint</code> in the IDE</li> <li>Verify that Python imports are sorted according to standard - run <code>make sort</code> in the IDE</li> <li>Python complexity checks: radon and xenon  - run <code>make complex</code> in the IDE</li> <li>Unit tests. Run <code>make unit</code> to run unit tests in the IDE</li> <li>Infrastructure test. Run <code>make infra-tests</code> to run the CDK infrastructure tests in the IDE</li> <li>Code coverage by codecov.io</li> <li>Deploy CDK - run <code>make deploy</code> in the IDE, will also run security tests based on cdk_nag</li> <li>Integration tests - run <code>make integration</code> in the IDE.</li> <li>E2E tests  - run <code>make e2e</code> in the IDE</li> <li>Code coverage tests  - run <code>make coverage-tests</code> in the IDE after CDK dep</li> <li>Update GitHub documentation branch</li> <li>Update dependencies and CDK version automatically to the latest versions: run <code>make update-deps</code>.</li> </ul>"},{"location":"pipeline/#other-capabilities","title":"Other Capabilities","text":"<ul> <li>Automatic Python dependencies update with Dependabot</li> <li>Easy to use makefile allows to run locally all commands in the GitHub actions</li> <li>Run local docs server, prior to push in pipeline - run <code>make docs</code>  in the IDE</li> <li>Prepare PR, run all checks with one command - run <code>make pr</code> in the IDE</li> </ul>"},{"location":"pipeline/#environments-pipelines","title":"Environments &amp; Pipelines","text":"<p>All GitHub workflows are stored under <code>.github/workflows</code> folder.</p> <p>The two most important ones are <code>pr-serverless-service</code>  and <code>main-serverless-service</code>.</p>"},{"location":"pipeline/#pr-serverless-service","title":"pr-serverless-service","text":"<p><code>pr-serverless-service</code> runs for every pull request you open. It expects you defined a GitHub environments by the name <code>dev</code>, <code>staging</code> and <code>production</code> and for each environments to have the following variables:  <code>CODECOV_TOKEN</code> for CodeCov integration and <code>AWS_ROLE</code> that allows GitHub to deploy to that AWS account (one for dev, one for staging and one for production accounts).</p> <p>It includes two jobs: 'quality_standards' and 'tests' where a failure in 'quality_standards' does not trigger 'tests'. Both jobs MUST pass in order to to be able to merge.</p> <p>'quality_standards' includes all linters, pre-commit checks and units tests and 'tests' deploys the service to AWS, runs code coverage checks,</p> <p>checks that your OpenAPI file is up to date and corresponds to the actual deployment (fails if it does not, hence your documentation is out of date), security checks and E2E tests. Stack is destroyed at the end. Stack has a 'dev'</p> <p>prefix as part of its name. Each environment has a pre-defined stack prefix.</p> <p>Once merged, <code>main-serverless-service</code> will run.</p>"},{"location":"pipeline/#main-serverless-service","title":"main-serverless-service","text":"<p><code>main-serverless-service</code> runs for every MERGED pull request that runs on the main branch. It expects you defined a GitHub environments by the name <code>staging</code> and <code>production</code> and that both includes a secret by the name of <code>AWS_ROLE</code>.</p> <p>It includes three jobs: 'staging', 'production' and 'publish_github_pages'.</p> <p>'staging' does not run any of the 'quality_standards' checks, since they were already checked before the code was merged. It runs just coverage tests and E2E tests. Stack is not deleted. Stack has a 'staging' prefix as part of its name. Any failure in staging will stop the pipeline and production environment will not get updated with the new code.</p> <p>'production' does not run any of the 'quality_standards' checks, since they were already checked before the code was merged. It does not run any test at the moment. Stack is not deleted. Stack has a 'production' prefix as part of its name.</p>"},{"location":"best_practices/environment_variables/","title":"Environment Variables","text":"<p>Environment Variables decorator is a simple parser for environment variables that run at the start of the handler invocation.</p> <p></p>"},{"location":"best_practices/environment_variables/#key-features","title":"Key features","text":"<ul> <li>A defined Pydantic schema for all required environment variables</li> <li>A decorator that parses and validates environment variables, value constraints included</li> <li>Global getter for parsed &amp; valid schema dataclass with all environment variables</li> </ul> <p>The best practice for handling environment variables is to validate &amp; parse them according to a predefined schema as soon as the AWS Lambda function is triggered.</p> <p>In case of misconfiguration, a validation exception is raised with all the relevant exception details.</p>"},{"location":"best_practices/environment_variables/#open-source","title":"Open source","text":"<p>The code in this post has been moved to an open source project you can use:</p> <p>The AWS Lambda environment variables modeler</p>"},{"location":"best_practices/environment_variables/#blog-reference","title":"Blog Reference","text":"<p>Read more about the importance of validating environment variables and how this utility works. Click HERE</p>"},{"location":"best_practices/environment_variables/#schema-definition","title":"Schema Definition","text":"<p>You need to define all your environment variables in a Pydantic schema class that extend Pydantic's BaseModel class.</p> <p>For example:</p> schemas/env_vars.py<pre><code>from typing import Annotated, Literal\n\nfrom pydantic import BaseModel, Field, HttpUrl\n\n\nclass MyHandlerEnvVars(BaseModel):\n    REST_API: HttpUrl\n    ROLE_ARN: Annotated[str, Field(min_length=20, max_length=2048)]\n    POWERTOOLS_SERVICE_NAME: Annotated[str, Field(min_length=1)]\n    LOG_LEVEL: Literal['DEBUG', 'INFO', 'ERROR', 'CRITICAL', 'WARNING', 'EXCEPTION']\n</code></pre> <p>All Pydantic schemas extend Pydantic\u2019s \u2018BaseModel\u2019 class, turning them into a dataclass.</p> <p>The schema defines four environment variables: \u2018LOG_LEVEL,\u2019 \u2018POWERTOOLS_SERVICE_NAME,\u2019 \u2018ROLE_ARN,\u2019 and \u2018REST_API.\u2019</p> <p>This schema makes sure that:</p> <ul> <li>\u2018LOG_LEVEL\u2019 is one of the strings in the Literal list.</li> <li>\u2018ROLE_ARN\u2019 exists and is between 20 and 2048 characters long, as defined here.</li> <li>\u2018REST_API\u2019 is a valid HTTP URL.</li> <li>\u2018POWERTOOLS_SERVICE_NAME\u2019 is a non-empty string.</li> </ul> <p>Read here about Pydantic Model capabilities.</p>"},{"location":"best_practices/environment_variables/#decorator-usage","title":"Decorator Usage","text":"<p>The decorator 'init_environment_variables' is defined under the utility folder service.utils.env_vars_parser.py and imported in the handler.</p> <p>The decorator requires a model parameter, which in this example is the name of the schema class we defined above.</p> handlers/my_handler.py<pre><code>import json\nfrom http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_env_modeler import get_environment_variables, init_environment_variables\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nfrom service.handlers.models.env_vars import McpHandlerEnvVars\n\n\n@init_environment_variables(model=McpHandlerEnvVars)\ndef my_handler(event: dict[str, Any], context: LambdaContext) -&gt; dict[str, Any]:\n    env_vars = get_environment_variables(model=McpHandlerEnvVars)  # noqa: F841\n    return {'statusCode': HTTPStatus.OK, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'message': 'success'})}\n</code></pre>"},{"location":"best_practices/environment_variables/#global-getter-usage","title":"Global Getter Usage","text":"<p>The getter function 'get_environment_variables' is defined under the utility folder service.utils.env_vars_parser.py and imported in the handler.</p> <p>The getter function returns a parsed and validated global instance of the environment variables Pydantic schema class.</p> <p>It can be used anywhere in the function code, not just the handler.</p> handlers/my_handler.py<pre><code>import json\nfrom http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_env_modeler import get_environment_variables, init_environment_variables\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nfrom service.handlers.models.env_vars import McpHandlerEnvVars\n\n\n@init_environment_variables(model=McpHandlerEnvVars)\ndef my_handler(event: dict[str, Any], context: LambdaContext) -&gt; dict[str, Any]:\n    env_vars: McpHandlerEnvVars = get_environment_variables(model=McpHandlerEnvVars)  # noqa: F841\n    return {'statusCode': HTTPStatus.OK, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'message': 'success'})}\n</code></pre>"},{"location":"best_practices/environment_variables/#more-details","title":"More Details","text":"<p>Read here about Pydantic field types.</p> <p>Read here about custom validators and advanced value constraints.</p>"},{"location":"best_practices/input_validation/","title":"Input Validation","text":"<p>This utility provides input validation and advanced parsing. It mitigates any hidden input assumption including value constraints.</p> <p>Rule of thumb: Always, always, always (!), validate all input.</p> <p></p>"},{"location":"best_practices/input_validation/#key-features","title":"Key features","text":"<p>The Parser will validate the incoming event, extract the input business payload, decode it and validate it according to a predefined schema.</p> <p>This schema will verify that all required parameters exist, their type is as expected and validate any value constraints.</p> <p>And all this will be achieved with one line of code.</p> <p>This \"magic\" line will allow you to focus on your business logic and not worry about metadata and encapsulating services.</p>"},{"location":"best_practices/input_validation/#service-envelope","title":"Service Envelope","text":"<p>When an AWS Service sends an event that triggers your AWS Lambda function, metadata information is added to the event, and the business logic payload is encapsulated.</p> <p>Let's call this metadata information 'envelope.'</p> <p>The envelope contains valuable information, interesting headers, and the interesting part, the business logic input that you wish to process.</p> <p>That's where it gets tricky.</p> <p>Each AWS service has its envelope structure and may encapsulate the business logic payload differently.</p>"},{"location":"best_practices/input_validation/#supported-aws-services","title":"Supported AWS Services","text":"<p>For a complete list click here.</p>"},{"location":"best_practices/input_validation/#define-business-logic-schema","title":"Define Business Logic Schema","text":"<p>Use Pydantic schemas to define the expected input format. Extend 'BaseModel' class.</p> <p>Define type and value constraints.</p> schemas/input.py <pre><code>from pydantic import BaseModel, PositiveInt, constr\n\nclass Input(BaseModel):\n    customer_name: constr(min_length=1, max_length=20)\n    order_item_count: PositiveInt\n</code></pre> <p>The schema defines:</p> <ol> <li>'customer_name' - customer name, a non-empty string with up to 20 characters.</li> <li>'order_item_count' - a positive integer representing the number of ordered items that 'customer_name' placed.</li> </ol> <p>Learn about models here and about advanced parsing here.</p>"},{"location":"best_practices/input_validation/#usage-in-handler","title":"Usage in Handler","text":"<p>The parser is a called with the function 'parse'.</p> <p>Use the envelope class that matches the AWS service that triggers your AWS Lambda function.</p> my_handler.py<pre><code>from http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_powertools.utilities.parser import ValidationError, parse\nfrom aws_lambda_powertools.utilities.parser.envelopes import ApiGatewayEnvelope\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nfrom .schema import Input\n\n\ndef my_handler(event: dict[str, Any], context: LambdaContext):\n    try:\n        input: Input = parse(event=event, model=Input, envelope=ApiGatewayEnvelope)  # noqa: F841\n    except (ValidationError, TypeError):\n        # log error, return BAD_REQUEST\n        return {'statusCode': HTTPStatus.BAD_REQUEST}\n    # process input\n</code></pre>"},{"location":"best_practices/input_validation/#accessing-envelope-metadata","title":"Accessing Envelope Metadata","text":"<p>You can access the metadata parameters by extending the model class and parsing the input without the envelope class.</p> <p>Read more about it here.</p>"},{"location":"best_practices/input_validation/#blog-reference","title":"Blog Reference","text":"<p>Read more about the importance of input validation and the potential pitfalls it prevents in my blog. Click HERE.</p>"},{"location":"best_practices/input_validation/#more-details","title":"More Details","text":"<p>You can find more information at the official documentation.</p> <p>Go to https://docs.powertools.aws.dev/lambda-python/latest/utilities/parser/</p>"},{"location":"best_practices/logger/","title":"Logger","text":"<p>It\u2019s a wrapper of Python\u2019s logging library that provides extra capabilities such as JSON output configuration (and many more).</p> <p></p>"},{"location":"best_practices/logger/#key-features","title":"Key features","text":"<ul> <li>Capture key fields from Lambda context, cold start and structures logging output as JSON</li> <li>Log Lambda event when instructed (disabled by default)</li> <li>Append additional keys to structured log at any point in time</li> </ul>"},{"location":"best_practices/logger/#usage-in-handler","title":"Usage in Handler","text":"my_handler.py<pre><code>import json\nfrom http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_powertools.logging.logger import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger: Logger = Logger(service='service')  # JSON output format, service name can be set by environment variable \"POWERTOOLS_SERVICE_NAME\"\n\n\ndef my_handler(event: dict[str, Any], context: LambdaContext) -&gt; dict[str, Any]:\n    logger.set_correlation_id(context.aws_request_id)\n    logger.debug('my_handler is called')\n    return {'statusCode': HTTPStatus.OK, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'message': 'success'})}\n</code></pre>"},{"location":"best_practices/logger/#blog-reference","title":"Blog Reference","text":"<p>Read more about the importance of the logger and how to use AWS CloudWatch logs in my blog. Click HERE</p>"},{"location":"best_practices/logger/#more-details","title":"More Details","text":"<p>You can find more information at the official documentation.</p> <p>Go to https://docs.powertools.aws.dev/lambda-python/latest/core/logger/</p>"},{"location":"best_practices/metrics/","title":"Metrics","text":"<p>Business metrics and KPIs can drive your business forward towards success.</p> <p>Metrics consist of a key and a value; A value can be a number, percentage, rate, or any other unit. Typical metrics are the number of sessions, users, error rate, number of views, etc.</p> <p>The Metrics utility creates custom metrics asynchronously by logging metrics to standard output following Amazon CloudWatch Embedded Metric Format (EMF).</p> <p>These metrics can be visualized through Amazon CloudWatch Console.</p> <p></p>"},{"location":"best_practices/metrics/#key-features","title":"Key features","text":"<ul> <li>Aggregate up to 100 metrics using a single CloudWatch EMF object (large JSON blob)</li> <li>Validate against common metric definitions mistakes (metric unit, values, max dimensions, max metrics, etc)</li> <li>Metrics are created asynchronously by CloudWatch service, no custom stacks needed</li> </ul>"},{"location":"best_practices/metrics/#usage-in-handler","title":"Usage in Handler","text":"my_handler.py<pre><code>import json\nfrom http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_powertools.metrics import Metrics, MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nSERVICE_NAME = 'service'\n\n# namespace and service name can be set by environment variable \"POWERTOOLS_METRICS_NAMESPACE\" and \"POWERTOOLS_SERVICE_NAME\" accordingly\nmetrics = Metrics(namespace='my_product_kpi', service=SERVICE_NAME)\n\n\n@metrics.log_metrics\ndef my_handler(event: dict[str, Any], context: LambdaContext) -&gt; dict[str, Any]:\n    metrics.add_metric(name='ValidEvents', unit=MetricUnit.Count, value=1)\n    return {'statusCode': HTTPStatus.OK, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'message': 'success'})}\n</code></pre>"},{"location":"best_practices/metrics/#blog-reference","title":"Blog Reference","text":"<p>Read more about the importance of the business KPis and metrics in my blog. Click HERE</p>"},{"location":"best_practices/metrics/#more-details","title":"More Details","text":"<p>You can find more information at the official documentation.</p> <p>Go to https://docs.powertools.aws.dev/lambda-python/latest/core/metrics/</p>"},{"location":"best_practices/monitoring/","title":"Monitoring","text":""},{"location":"best_practices/monitoring/#key-concepts","title":"Key Concepts","text":"<p>Utilizing AWS CloudWatch dashboards enables centralized monitoring of API Gateway, Lambda functions, and DynamoDB, providing real-time insights into their performance and operational health. By aggregating metrics, logs, and alarms, CloudWatch facilitates swift issue diagnosis and analysis across your serverless applications. Additionally, setting up alarms ensures immediate alerts during anomalous activities, enabling proactive issue mitigation.</p>"},{"location":"best_practices/monitoring/#service-architecture","title":"Service Architecture","text":"<p>The goal is to monitor the service API gateway, Lambda function, and DynamoDB tables and ensure everything is in order.</p> <p>In addition, we want to visualize service KPI metrics.</p>"},{"location":"best_practices/monitoring/#monitoring-dashboards","title":"Monitoring Dashboards","text":"<p>We will define two dashboards:</p> <ul> <li>High level</li> <li>Low level</li> </ul> <p>Each dashboard has its usage and tailors different personas' usage.</p>"},{"location":"best_practices/monitoring/#high-level-dashboard","title":"High Level Dashboard","text":"<p>This dashboard is designed to be an executive overview of the service.</p> <p>Total API gateway metrics provide information on the performance and error rate of the service.</p> <p>KPI metrics are included in the bottom part as well.</p> <p>Personas that use this dashboard: SRE, developers, and product teams (KPIs)</p>"},{"location":"best_practices/monitoring/#low-level-dashboard","title":"Low Level Dashboard","text":"<p>It is aimed at a deep dive into all the service's resources. Requires an understanding of the service architecture and its moving parts.</p> <p>The dashboard provides the Lambda function's metrics for latency, errors, throttles, provisioned concurrency, and total invocations.</p> <p>In addition, a CloudWatch logs widget shows only 'error' logs from the Lambda function.</p> <p>As for DynamoDB tables, we have the primary database and the idempotency table for usage, operation latency, errors, and throttles.</p> <p>Personas that use this dashboard: developers, SREs.</p>"},{"location":"best_practices/monitoring/#alarms","title":"Alarms","text":"<p>Having visibility and information is one thing, but being proactive and knowing beforehand that a significant error is looming is another. A CloudWatch</p> <p>Alarm is an automated notification tool within AWS CloudWatch that triggers alerts based on user-defined thresholds, enabling users to identify and</p> <p>respond to operational issues, breaches, or anomalies in AWS resources by monitoring specified metrics over a designated period.</p> <p>In this dashboard, you will find an example of two types of alarms:</p> <ul> <li>Alarm for performance threshold monitoring</li> <li>Alarm for error rate threshold monitoring</li> </ul> <p>For latency-related issues, we define the following alarm:</p> <p></p> <p>For P90, P50 metrics, follow this explanation.</p> <p>For internal server errors rate, we define the following alarm: </p>"},{"location":"best_practices/monitoring/#actions","title":"Actions","text":"<p>Alarms are of no use unless they have an action. We have configured the alarms to send an SNS notification to a new SNS topic. From there, you can connect any subscription - HTTPS/SMS/Email, etc. to notify your teams with the alarm details.</p>"},{"location":"best_practices/monitoring/#cdk-reference","title":"CDK Reference","text":"<p>We use the open-source cdk-monitoring-constructs.</p> <p>You can view find the monitoring CDK construct here.</p>"},{"location":"best_practices/monitoring/#further-reading","title":"Further Reading","text":"<p>If you wish to learn more about this concept and go over details on the CDK code, check out my blog post.</p>"},{"location":"best_practices/tracer/","title":"Tracer","text":"<p>Tracer is a thin wrapper for AWS X-Ray Python SDK.</p> <p></p>"},{"location":"best_practices/tracer/#key-features","title":"Key features","text":"<ul> <li>Enables AWS X-Ray traces for your handler by using simple and clean decorators</li> <li>Captures handler and inner functions</li> <li>Auto capture cold start as annotation, and responses or full exceptions as metadata</li> </ul>"},{"location":"best_practices/tracer/#usage-in-handler","title":"Usage in Handler","text":"my_handler.py<pre><code>import json\nfrom http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_powertools.tracing.tracer import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nSERVICE_NAME = 'service'\n\n# service name can be set by environment variable \"POWERTOOLS_SERVICE_NAME\". Disabled by setting POWERTOOLS_TRACE_DISABLED to \"True\"\ntracer: Tracer = Tracer(service=SERVICE_NAME)\n\n\n@tracer.capture_method(capture_response=False)\ndef inner_function_example(event: dict[str, Any]) -&gt; dict[str, Any]:\n    return {}\n\n\n@tracer.capture_lambda_handler(capture_response=False)\ndef my_handler(event: dict[str, Any], context: LambdaContext) -&gt; dict[str, Any]:\n    inner_function_example(event)\n    return {'statusCode': HTTPStatus.OK, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'message': 'success'})}\n</code></pre>"},{"location":"best_practices/tracer/#blog-reference","title":"Blog Reference","text":"<p>Read more about the importance of observability and traces in my blog. Click HERE</p>"},{"location":"best_practices/tracer/#more-details","title":"More Details","text":"<p>You can find more information at the official documentation.</p> <p>Go to https://docs.powertools.aws.dev/lambda-python/latest/core/tracer/</p>"}]}